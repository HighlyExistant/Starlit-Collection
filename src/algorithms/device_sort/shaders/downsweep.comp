#version 450
#extension GL_EXT_shader_atomic_float : require
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_shader_subgroup_ballot: enable
#extension GL_KHR_shader_subgroup_basic: enable
#extension GL_ARB_shader_ballot: enable
#define RADIX                       256
#define HALF_RADIX                  128
#define RADIX_LOG                   8
#define RADIX_MASK                  255
#define RADIX_START_SECOND          256
#define RADIX_START_THIRD           512
#define RADIX_START_FOURTH          768
#define PART_SIZE                   7680
#define D_TOTAL_SMEM                7936
#define FLAG_REDUCTION              1
#define FLAG_INCLUSIVE              2
#define FLAG_MASK                   3       //Mask used to retrieve flag values
#define KEYS_PER_THREAD             15
#define D_DIM                       512

shared uint g_d[D_TOTAL_SMEM];

struct KeyStruct {
    uint k[KEYS_PER_THREAD];
};
struct OffsetStruct {
    uint o[KEYS_PER_THREAD];
};

layout (local_size_x = D_DIM, local_size_y = 1, local_size_z = 1) in;

layout(binding = 0) buffer GlobalHistogram {
    uint val[];
} global_histogram;
layout(binding = 1) buffer HistogramPass {
    uint val[];
} histpass;
layout(binding = 2) buffer HistogramSort {
    uint val[];
} sortbuf;
layout(binding = 3) buffer AlternateBuffer {
    uint val[];
} altbuf;
layout(push_constant, std430) uniform PushConstant {
    uint num_keys;
    uint threadblocks;
    uint radix_shift;
} pc;
uint getWaveIndex() {
    return gl_LocalInvocationID.x / gl_SubgroupSize;
}
uint current_pass() {
    return pc.radix_shift >> 3;
}
uint wave_histogram_size_wge16() {
    return D_DIM / gl_SubgroupSize * RADIX;
}
uint wave_histogram_size_wlt16()
{
    return D_TOTAL_SMEM;
}
uint pass_histogram_offset(uint index) {
    return ((current_pass() * pc.threadblocks) + index) << RADIX_LOG;
}
void clear_wave_histogram() {
    uint hist_end = gl_SubgroupSize >= 16 ?
        wave_histogram_size_wge16() : wave_histogram_size_wlt16();
    for (uint i = gl_LocalInvocationID.x; i < hist_end; i += D_DIM) {
        g_d[i] = 0;
    }
}
uint subpart_size_wge16() {
    return KEYS_PER_THREAD * gl_SubgroupSize;
}
uint shared_offset_wge16() {
    return gl_SubgroupInvocationID.x + getWaveIndex() * subpart_size_wge16();
}
uint device_offset_wge16(uint part_index) {
    return shared_offset_wge16() + part_index * PART_SIZE;
}
void load_key(inout uint key, uint index) {
    key = sortbuf.val[index];
}
void load_dummy_key(inout uint key) {
    key = 0xffffffff;
}
KeyStruct load_keys_wge16(uint part_index) {
    KeyStruct keys;
    for (uint i = 0, t = device_offset_wge16(part_index);
        i < KEYS_PER_THREAD;
        ++i, t += gl_SubgroupSize) {
        load_key(keys.k[i], t);
    }
    return keys;
}
KeyStruct load_keys_partial_wge16(uint part_index) {
    KeyStruct keys;
    for (uint i = 0, t = device_offset_wge16(part_index);
        i < KEYS_PER_THREAD;
        i++, t += gl_SubgroupSize) {
            if (t < pc.num_keys) {
                load_key(keys.k[i], t);
            } else {
                load_dummy_key(keys.k[i]);
            }
    }
    return keys;
}
uint wave_flags_wge16() {
    return (bool(gl_SubgroupSize & 31)) ?
        (1 << gl_SubgroupSize) - 1 : 0xffffffff;
}
void warp_level_multisplit_wge16(uint key, uint wave_parts, inout uvec4 wave_flags) {
    for (uint k = 0; k < RADIX_LOG; ++k)
    {
        const bool t = bool(key >> (k + pc.radix_shift) & 1);
        const uvec4 ballot = subgroupBallot(t);
        for (uint wave_part = 0; wave_part < wave_parts; ++wave_part)
            wave_flags[wave_part] &= (t ? 0 : 0xffffffff) ^ ballot[wave_part];
    }
}
uint extract_digit(uint key)
{
    return key >> pc.radix_shift & RADIX_MASK;
}
uint find_lowest_ranking_peer(uvec4 wave_flags, uint wave_parts) {
    uint lowest_rank_peer = 0;
    for (uint wave_part = 0; wave_part < wave_parts; ++wave_part)
    {
        uint fbl = findLSB(wave_flags[wave_part]);
        if (fbl == 0xffffffff)
            lowest_rank_peer += 32;
        else
            return lowest_rank_peer + fbl;
    }
    return 0; //will never happen
}
void count_peer_bits(
    inout uint peer_bits,
    inout uint total_bits,
    uvec4 wave_flags,
    uint wave_parts) {
    for (uint wave_part = 0; wave_part < wave_parts; ++wave_part) {
        if (gl_SubgroupInvocationID.x >= wave_part * 32) {
            uint ltmask = gl_SubgroupInvocationID.x >= (wave_part + 1) * 32 ?
                0xffffffff : (1 << (gl_SubgroupInvocationID.x & 31)) - 1;
            peer_bits += bitCount(wave_flags[wave_part] & ltmask);
        }
        total_bits += bitCount(wave_flags[wave_part]);
    }
}
// TODO
OffsetStruct rank_keys_wge16(KeyStruct keys) {
    OffsetStruct offsets;
    uint wave_parts = (gl_SubgroupSize + 31)/32;
    for (uint i = 0; i < KEYS_PER_THREAD; ++i) {
        uvec4 wave_flags = uvec4(wave_flags_wge16());
        warp_level_multisplit_wge16(keys.k[i], wave_parts, wave_flags);

        uint index = extract_digit(keys.k[i]) + (getWaveIndex() * RADIX);
        uint lowest_rank_peer = find_lowest_ranking_peer(wave_flags, wave_parts);

        uint peer_bits = 0;
        uint total_bits = 0;
        count_peer_bits(peer_bits, total_bits, wave_flags, wave_parts);

        uint pre_increment_val;
        if (peer_bits == 0) {
            pre_increment_val = atomicAdd(g_d[index], total_bits);
        }
        offsets.o[i] = readInvocationARB(pre_increment_val, lowest_rank_peer) + peer_bits;
    }
    return offsets;
}
uint wave_hist_inclusive_scan_circular_shift_wge16() {
    
    uint hist_reduction = g_d[gl_LocalInvocationID.x];
    for (uint i = gl_LocalInvocationID.x + RADIX; i < wave_histogram_size_wge16(); i += RADIX) {
        hist_reduction += g_d[i];
        g_d[i] = hist_reduction - g_d[i];
    }
    return hist_reduction;
}
void wave_hist_reduction_exclusive_scan_wge16(uint hist_reduction) {
    if (gl_LocalInvocationID.x < RADIX) {
        uint lane_mask = gl_SubgroupSize-1;
        g_d[((gl_SubgroupInvocationID.x + 1) & lane_mask) + (gl_LocalInvocationID.x & ~lane_mask)] = hist_reduction;
    }
    groupMemoryBarrier();
    barrier();

    if (gl_LocalInvocationID.x < RADIX / gl_SubgroupSize) {
        g_d[gl_LocalInvocationID.x * gl_SubgroupSize] =
            subgroupExclusiveAdd(g_d[gl_LocalInvocationID.x * gl_SubgroupSize]);
    }
    groupMemoryBarrier();
    barrier();
    if (gl_LocalInvocationID.x < RADIX && bool(gl_SubgroupInvocationID.x)) {
        g_d[gl_LocalInvocationID.x] += readInvocationARB(g_d[gl_LocalInvocationID.x - 1], 1);
    }
}
void update_offsets_wge16(inout OffsetStruct offsets, KeyStruct keys) {
    if (gl_LocalInvocationID.x >= gl_SubgroupSize) {
        uint t = getWaveIndex()*RADIX;
        for (uint i = 0; i < KEYS_PER_THREAD; ++i) {
            uint t2 = extract_digit(keys.k[i]);
            offsets.o[i] += g_d[t2 + t] + g_d[t2];
        }
    } else {
        for (uint i = 0; i < KEYS_PER_THREAD; ++i) {
            offsets.o[i] += g_d[extract_digit(keys.k[i])];
        }
    }
}
void scatter_keys_shared(OffsetStruct offsets, KeyStruct keys) {
    
    for (uint i = 0; i < KEYS_PER_THREAD; ++i) {
        g_d[offsets.o[i]] = keys.k[i];
    }
}
void write_key(uint device_index, uint group_shared_index) {
    altbuf.val[device_index] = g_d[group_shared_index];
}
uint descending_index(uint device_index) {
    return pc.num_keys - device_index - 1;
}
void scatter_keys_only_device_ascending()
{
    for (uint i = gl_LocalInvocationID.x; i < PART_SIZE; i += D_DIM)
        write_key(g_d[extract_digit(g_d[i]) + PART_SIZE] + i, i);
}
void scatter_keys_only_device_descending()
{
    if(pc.radix_shift == 24)
    {
        for (uint i = gl_LocalInvocationID.x; i < PART_SIZE; i += D_DIM)
            write_key(descending_index(g_d[extract_digit(g_d[i]) + PART_SIZE] + i), i);
    }
    else
    {
        scatter_keys_only_device_ascending();
    }
}
void scatter_keys_only_device() {
    scatter_keys_only_device_descending();
}
void scatter_device(uint part_index, OffsetStruct offsets) {
    scatter_keys_only_device();
}
void scatter_keys_only_device_partial_ascending(uint final_part_size)
{
    uint _i_ = 0;
    for (uint i = gl_LocalInvocationID.x; i < PART_SIZE; i += D_DIM)
    {
        if (i < final_part_size) {
            write_key(g_d[extract_digit(g_d[i]) + PART_SIZE] + i, i);
        }
        _i_++;
    }
}
void scatter_keys_only_device_partial_descending(uint final_part_size)
{
    if (pc.radix_shift == 24)
    {
        for (uint i = gl_LocalInvocationID.x; i < PART_SIZE; i += D_DIM)
        {
            if (i < final_part_size) {
                write_key(descending_index(g_d[extract_digit(g_d[i]) + PART_SIZE] + i), i);
            }
        }
    }
    else
    {
        scatter_keys_only_device_partial_ascending(final_part_size);
    }
}
void scatter_keys_only_device_partial(uint part_index) {
    uint final_part_size = pc.num_keys - part_index * PART_SIZE;
    scatter_keys_only_device_partial_descending(final_part_size);
}
void scatter_device_partial(uint part_index, OffsetStruct offsets) {
    scatter_keys_only_device_partial(part_index);
}
uint global_histogram_offset() {
    return pc.radix_shift<<5;
}
void load_thread_block_reductions(uint exclusive_hist_reduction) {
    if (gl_LocalInvocationID.x < RADIX)
    {
        g_d[gl_LocalInvocationID.x + PART_SIZE] = global_histogram.val[gl_LocalInvocationID.x + global_histogram_offset()] +
            histpass.val[gl_LocalInvocationID.x * pc.threadblocks + gl_WorkGroupID.x] - exclusive_hist_reduction;
    }
}
void main() {
    KeyStruct keys;
    OffsetStruct offsets;

    clear_wave_histogram();

    if (gl_WorkGroupID.x < pc.threadblocks-1) {
        if (gl_SubgroupSize >= 16) {
            keys = load_keys_wge16(gl_WorkGroupID.x);
        }
    }
    if (gl_WorkGroupID.x == pc.threadblocks-1) {
        if (gl_SubgroupSize >= 16) {
            keys = load_keys_partial_wge16(gl_WorkGroupID.x);
        }
    }
    uint exclusive_hist_reduction;
    if (gl_SubgroupSize >= 16) {
        groupMemoryBarrier();
        barrier();

        offsets = rank_keys_wge16(keys);
        groupMemoryBarrier();
        barrier();
        
        uint hist_reduction;
        if (gl_LocalInvocationID.x < RADIX) {
            hist_reduction = wave_hist_inclusive_scan_circular_shift_wge16();
            hist_reduction += subgroupExclusiveAdd(hist_reduction);
        }
        groupMemoryBarrier();
        barrier();
        wave_hist_reduction_exclusive_scan_wge16(hist_reduction);
        
        groupMemoryBarrier();
        barrier();

        update_offsets_wge16(offsets, keys);
        if (gl_LocalInvocationID.x < RADIX) {
            exclusive_hist_reduction = g_d[gl_LocalInvocationID.x];
        }
        
        groupMemoryBarrier();
        barrier();
    }
    scatter_keys_shared(offsets, keys);
    load_thread_block_reductions(exclusive_hist_reduction);
    groupMemoryBarrier();
    barrier();
    if (gl_WorkGroupID.x < pc.threadblocks-1) {
        scatter_device(gl_WorkGroupID.x, offsets);
    }
    if (gl_WorkGroupID.x == pc.threadblocks-1) {
        scatter_device_partial(gl_WorkGroupID.x, offsets);
    }
}